# -*- coding: utf-8 -*-
#import logging
#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
from gensim import corpora, models, similarities
from nltk.tokenize import word_tokenize
import codecs
##################### This is to calculate the similarity between 1 essays and other essays

# 中文注释
# Read the text
# If the texts are stored in different .txt
'''courses = [line.strip() for line in file('coursera_corpus')]
courses_name = [course.split('\t')[0] for course in courses] # The structure is coursename\tcourse intro\tcourse detail
print courses_name[0:10]
'''

# Input：Texts from one teacher. A text from a student.
# Output: The average similarity between this teacher's essays and the student's essays
# argv: handin filename; teacher's filename1; teacher's filename2

'''
["Shipment of gold damaged in a fire",
... "Delivery of silver arrived in a silver truck",
... "Shipment of gold arrived in a truck"]
'''
def Similarity():
    print('similarity1')
    ori_handin = "Kate likes music very much and she likes playing the piano"
    ori_texts = ['Piano is a kind of music instrument','There is a car near the river']

    # Preprocess of the text
    texts = [[word for word in text.lower().split()] for text in ori_texts]
    handin = [word for word in ori_handin.lower().split()]
        #texts = NLTK_essays.Preprocess_Essays(essays)
    print('similarity2')

    # Use gensim
    # Dictionarize
    dictionary = corpora.Dictionary(texts)
    print('dictionary:');
    print(dictionary)
    print('dictionary.token2id:');
    print(dictionary.token2id)
    corpus = [dictionary.doc2bow(text) for text in texts]
    print('corpus')
    print(corpus)

    # Tf-idf Model
    tfidf = models.TfidfModel(corpus)
    corpus_tfidf = tfidf[corpus]
    print('corpus_tfidf')
    for i in corpus_tfidf:
        print(i)
    print('similarity3')
    # Set and train the LSI model
    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=20)

    corpus_lsi = lsi[corpus_tfidf]
    print('corpus_lsi')
    for i in corpus_lsi:
        print(i)
    # The essay handed in by the student
    ml_handin = handin
    #ml_handin = NLTK_handin.Preprocess_Handin(handin)
    ml_bow = dictionary.doc2bow(ml_handin)
    print('ml_bow')
    print(ml_bow)
    ml_lsi = lsi[ml_bow]
    print('ml_lsi:')
    print(ml_lsi)

    print('similarity4')
    # The index matrix to store the lsi model of each topic. # The size of matrix here is 2 documents * 10 features
    index = similarities.MatrixSimilarity(lsi[corpus])
    sims = index[ml_lsi]  # Compare the lsi model of student's essay to the teacher's essay
    sort_sims = sorted(enumerate(sims), key=lambda item: -item[1])
    print('sort_sims:')
    print(sort_sims) # Form: (num, similarity)
    return sort_sims

if __name__ == '__main__':
    Similarity()